{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "z-F_j5xGJ_BJ",
   "metadata": {
    "id": "z-F_j5xGJ_BJ"
   },
   "source": [
    "# Sentiment Analysis of Tweets Dataset using the following Machine Learning models\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- Extra Trees Classifier\n",
    "- Ada Boost Classifier\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f965746",
   "metadata": {
    "id": "8f965746"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98da782e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98da782e",
    "outputId": "0892afd2-cfb6-4e95-8943-84de53e8ef6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\apfle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\apfle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\apfle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\apfle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download punctuation and stopwords from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wJuTq-P9LO38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJuTq-P9LO38",
    "outputId": "6ab8cfbb-3907-4f9f-d2f4-2d6eb5d94421"
   },
   "outputs": [],
   "source": [
    "# set up spark\n",
    "\n",
    "# import os\n",
    "# # Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# # For example:\n",
    "# spark_version = 'spark-3.2.3'\n",
    "# # spark_version = 'spark-3.<enter version>'\n",
    "# os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# # Install Spark and Java\n",
    "# !apt-get update\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "# !tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "# # Set Environment Variables\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# # Start a SparkSession\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c986391",
   "metadata": {
    "id": "4c986391"
   },
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c503a564",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "c503a564",
    "outputId": "eb21e477-f9e0-42e6-b1bd-10209bd39f91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tweets_df and view\n",
    "tweets_df = pd.read_csv(\"Resources/Tweets.csv\")\n",
    "\n",
    "\n",
    "# from pyspark import SparkFiles\n",
    "# url =\"https://tweet-2022.s3.amazonaws.com/Tweets.csv\"\n",
    "# spark.sparkContext.addFile(url)\n",
    "# spark_tweets_df = spark.read.csv(SparkFiles.get(\"Tweets.csv\"), sep=\",\", header=True)\n",
    "# tweets_df = spark.read.csv(SparkFiles.get(\"Tweets.csv\"), sep=\",\", header=True)\n",
    "\n",
    "\n",
    "# tweets_df = pd.read_csv(\"/content/Tweets.csv\")\n",
    "# tweets_df=spark_tweets_df.toPandas()\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c311757b",
   "metadata": {
    "id": "c311757b"
   },
   "outputs": [],
   "source": [
    "# get dataframe ready for processing\n",
    "\n",
    "# make sure the tweets in column \"text\" are strings\n",
    "tweets_df['text'] = tweets_df['text'].astype('str')\n",
    "\n",
    "# delete the unneccessary columns\n",
    "tweets_df = tweets_df.drop(columns=[\"textID\", \"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47021da3",
   "metadata": {
    "id": "47021da3"
   },
   "outputs": [],
   "source": [
    "def process_tweets(tweet):\n",
    "    # make the text all lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    tweet = \"\".join(char for char in tweet if char not in string.punctuation)\n",
    "    \n",
    "    # tokenize the tweet for url clean\n",
    "    tokenize_tweet_url = word_tokenize(tweet)\n",
    "    \n",
    "    # remove urls\n",
    "    tokenize_tweet_url = \" \".join([i for i in tokenize_tweet_url if 'http' not in i])\n",
    "    \n",
    "    # tokenize the tweet\n",
    "    tokenize_tweets = word_tokenize(tokenize_tweet_url)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    tweet_wo_stop = [word for word in tokenize_tweets if word not in stopword]\n",
    "    \n",
    "    # lemmatization\n",
    "    lemm = WordNetLemmatizer()\n",
    "    lemmed = [lemm.lemmatize(word) for word in tweet_wo_stop]\n",
    "    \n",
    "    # put string together\n",
    "    final_tweet = \" \".join(lemmed)\n",
    "    \n",
    "    return final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8616da88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8616da88",
    "outputId": "0f56fc3a-e9c8-4e07-806a-275c66bc425d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id responded going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bos bullying</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text sentiment\n",
       "0                      id responded going   neutral\n",
       "1                 sooo sad miss san diego  negative\n",
       "2                            bos bullying  negative\n",
       "3                   interview leave alone  negative\n",
       "4  son couldnt put release already bought  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process tweets using above function\n",
    "tweets_df['text'] = tweets_df['text'].apply(lambda x: process_tweets(x))\n",
    "tweets_df = tweets_df.dropna()\n",
    "\n",
    "# view updated dataframe\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2da464f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "f2da464f",
    "outputId": "c241a51e-4052-4f59-ed7d-fc4866c6e209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='sentiment'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize distribution\n",
    "tweets_df.sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=['blue', 'gold', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa959b52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aa959b52",
    "outputId": "8aab4aee-a69f-4ad5-f245-a4c70b3afceb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id responded going</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bos bullying</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text  sentiment\n",
       "0                      id responded going          0\n",
       "1                 sooo sad miss san diego         -1\n",
       "2                            bos bullying         -1\n",
       "3                   interview leave alone         -1\n",
       "4  son couldnt put release already bought         -1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the sentiment column into numbers\n",
    "dict_sentiment = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "tweets_df['sentiment'] = tweets_df['sentiment'].apply(lambda x: dict_sentiment.get(x))\n",
    "\n",
    "# view updated dataframe\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c926d3e2",
   "metadata": {
    "id": "c926d3e2"
   },
   "outputs": [],
   "source": [
    "# create a separate data frame without neutral tweets\n",
    "tweets_no_neut = tweets_df[tweets_df[\"sentiment\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GnRFkWIBKFU4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "GnRFkWIBKFU4",
    "outputId": "1ab5fad0-3636-4b7d-d4eb-968d20ff6a7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='sentiment'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize distribution - without neutral sentiment\n",
    "tweets_no_neut.sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=['blue', 'gold', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0280c2",
   "metadata": {
    "id": "7f0280c2"
   },
   "outputs": [],
   "source": [
    "# Create the X and y data \n",
    "def create_train_test_data (tweets_df, train_size):\n",
    "    \n",
    "    # assign X and y to the input and target columns\n",
    "    X = tweets_df['text']\n",
    "    y = tweets_df['sentiment']\n",
    "\n",
    "    # split the data into testing data and training data\n",
    "    if train_size == 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size)\n",
    "\n",
    "    # transform the data into tfidf vectors\n",
    "    # fit the tfidf vectorizer on the training data to avoid bias\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91767a3f",
   "metadata": {
    "id": "91767a3f"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "def create_model(tweets_df, attempt_num, model, train_size, notes):\n",
    "    # Get the train and test data\n",
    "    \n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = create_train_test_data (tweets_df, train_size)\n",
    "\n",
    "    # create a logistic regression model and fit it to the training data\n",
    "    \n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    training_score, testing_score, recall, precision = evaluate(model, X_train_tfidf, y_train, X_test_tfidf, y_test, attempt_num, notes)\n",
    "    \n",
    "    return training_score, testing_score, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0219b4d8",
   "metadata": {
    "id": "0219b4d8"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate(model, X_train_tfidf, y_train, X_test_tfidf, y_test, attempt_num, notes):\n",
    "    \n",
    "    training_score = model.score(X_train_tfidf, y_train)\n",
    "    testing_score = model.score(X_test_tfidf, y_test)\n",
    "    recall = recall_score(y_test.values, model.predict(X_test_tfidf), average='macro')\n",
    "    precision = precision_score(y_test.values, model.predict(X_test_tfidf), average='macro')\n",
    "    \n",
    "    # look at the scores for the testing and training data\n",
    "    print(f\"Attempt {attempt_num}: {notes} ----------------------------------------\")\n",
    "    print(f\"Training Data Score: {model.score(X_train_tfidf, y_train)}\")\n",
    "    print(f\"Testing Data Score: {model.score(X_test_tfidf, y_test)} \\n\")\n",
    "\n",
    "    # find metrics for testing data\n",
    "    print(confusion_matrix(y_test.values, model.predict(X_test_tfidf)))\n",
    "    print(classification_report(y_test.values, model.predict(X_test_tfidf)))\n",
    "    \n",
    "    return training_score, testing_score, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4Fu4frkthPzH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "4Fu4frkthPzH",
    "outputId": "ea3e0701-29fe-447e-d40e-aafb1a49c5dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Attempt Number</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Test_Size</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Attempt Number, Training Score, Testing Score, Parameters, Recall, Precision, Test_Size, Coefficient]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe for attempts\n",
    "performance_df = pd.DataFrame(columns=[\"Model\", \"Attempt Number\", \"Training Score\", \"Testing Score\", \"Parameters\", \"Recall\", \"Precision\", \"Test_Size\", \"Coefficient\"])\n",
    "performance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252b265",
   "metadata": {
    "id": "7252b265"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c056cb6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c056cb6b",
    "outputId": "f19a43d7-ec27-4d53-a846-5152f7f803f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apfle\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: 100 iterations, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.8256307617661329\n",
      "Testing Data Score: 0.6824342763576822 \n",
      "\n",
      "[[1814 1113  180]\n",
      " [ 602 3315  510]\n",
      " [ 143  943 2373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.58      0.64      3107\n",
      "           0       0.62      0.75      0.68      4427\n",
      "           1       0.77      0.69      0.73      3459\n",
      "\n",
      "    accuracy                           0.68     10993\n",
      "   macro avg       0.70      0.67      0.68     10993\n",
      "weighted avg       0.69      0.68      0.68     10993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apfle\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2: 100 iterations, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.8169577874818049\n",
      "Testing Data Score: 0.6871020556667273 \n",
      "\n",
      "[[ 914  563   92]\n",
      " [ 272 1639  270]\n",
      " [  69  454 1224]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.58      0.65      1569\n",
      "           0       0.62      0.75      0.68      2181\n",
      "           1       0.77      0.70      0.73      1747\n",
      "\n",
      "    accuracy                           0.69      5497\n",
      "   macro avg       0.71      0.68      0.69      5497\n",
      "weighted avg       0.70      0.69      0.69      5497\n",
      "\n",
      "Attempt 3: 100 iterations, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9293063053886116\n",
      "Testing Data Score: 0.8632752826153376 \n",
      "\n",
      "[[2656  392]\n",
      " [ 503 2995]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.87      0.86      3048\n",
      "           1       0.88      0.86      0.87      3498\n",
      "\n",
      "    accuracy                           0.86      6546\n",
      "   macro avg       0.86      0.86      0.86      6546\n",
      "weighted avg       0.86      0.86      0.86      6546\n",
      "\n",
      "Attempt 4: 100 iterations, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9317799847211612\n",
      "Testing Data Score: 0.8637335777574091 \n",
      "\n",
      "[[1347  215]\n",
      " [ 231 1480]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.86      0.86      1562\n",
      "           1       0.87      0.86      0.87      1711\n",
      "\n",
      "    accuracy                           0.86      3273\n",
      "   macro avg       0.86      0.86      0.86      3273\n",
      "weighted avg       0.86      0.86      0.86      3273\n",
      "\n",
      "Attempt 5: 300 iterations, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.8259946627850558\n",
      "Testing Data Score: 0.6807059037569362 \n",
      "\n",
      "[[1779 1209  158]\n",
      " [ 556 3391  544]\n",
      " [ 144  899 2313]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.57      0.63      3146\n",
      "           0       0.62      0.76      0.68      4491\n",
      "           1       0.77      0.69      0.73      3356\n",
      "\n",
      "    accuracy                           0.68     10993\n",
      "   macro avg       0.70      0.67      0.68     10993\n",
      "weighted avg       0.69      0.68      0.68     10993\n",
      "\n",
      "Attempt 6: 300 iterations, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.819641557496361\n",
      "Testing Data Score: 0.6903765690376569 \n",
      "\n",
      "[[ 949  542   91]\n",
      " [ 295 1686  283]\n",
      " [  60  431 1160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.60      0.66      1582\n",
      "           0       0.63      0.74      0.68      2264\n",
      "           1       0.76      0.70      0.73      1651\n",
      "\n",
      "    accuracy                           0.69      5497\n",
      "   macro avg       0.71      0.68      0.69      5497\n",
      "weighted avg       0.70      0.69      0.69      5497\n",
      "\n",
      "Attempt 7: 300 iterations, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9342976469389834\n",
      "Testing Data Score: 0.8640391078521235 \n",
      "\n",
      "[[2708  410]\n",
      " [ 480 2948]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.87      0.86      3118\n",
      "           1       0.88      0.86      0.87      3428\n",
      "\n",
      "    accuracy                           0.86      6546\n",
      "   macro avg       0.86      0.86      0.86      6546\n",
      "weighted avg       0.86      0.86      0.86      6546\n",
      "\n",
      "Attempt 8: 300 iterations, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9310924369747899\n",
      "Testing Data Score: 0.869233119462267 \n",
      "\n",
      "[[1352  209]\n",
      " [ 219 1493]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.87      0.86      1561\n",
      "           1       0.88      0.87      0.87      1712\n",
      "\n",
      "    accuracy                           0.87      3273\n",
      "   macro avg       0.87      0.87      0.87      3273\n",
      "weighted avg       0.87      0.87      0.87      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=100)\n",
    "model2 = LogisticRegression(max_iter=300)\n",
    "\n",
    "# models with 100 max iterations\n",
    "training_score1, testing_score1, recall1, precision1 = create_model(tweets_df, 1, model, .6, \"100 iterations, 60/40 split, including neutrals\")\n",
    "training_score2, testing_score2, recall2, precision2 = create_model(tweets_df, 2, model, .8, \"100 iterations, 80/20 split, including neutrals\")\n",
    "training_score3, testing_score3, recall3, precision3 = create_model(tweets_no_neut, 3, model, .6, \"100 iterations, 60/40 split, excluding neutrals\")\n",
    "training_score4, testing_score4, recall4, precision4 = create_model(tweets_no_neut, 4, model, .8, \"100 iterations, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 1, training_score1, testing_score1, 'including neutrals', recall1, precision1, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 2, training_score2, testing_score2, 'including neutrals', recall2, precision2, .8, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 3, training_score3, testing_score3, 'excluding neutrals', recall3, precision3, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 4, training_score4, testing_score4, 'excluding neutrals', recall4, precision4, .8, 100]\n",
    "\n",
    "# models with 300 max iterations\n",
    "training_score5, testing_score5, recall5, precision5 = create_model(tweets_df, 5, model2, .6, \"300 iterations, 60/40 split, including neutrals\")\n",
    "training_score6, testing_score6, recall6, precision6 = create_model(tweets_df, 6, model2, .8, \"300 iterations, 80/20 split, including neutrals\")\n",
    "training_score7, testing_score7, recall7, precision7 = create_model(tweets_no_neut, 7, model2, .6, \"300 iterations, 60/40 split, excluding neutrals\")\n",
    "training_score8, testing_score8, recall8, precision8 = create_model(tweets_no_neut, 8, model2, .8, \"300 iterations, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 5, training_score5, testing_score5, 'including neutrals', recall5, precision5, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 6, training_score6, testing_score6, 'including neutrals', recall6, precision6, .8, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 7, training_score7, testing_score7, 'excluding neutrals', recall7, precision7, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Logistic Regression', 8, training_score8, testing_score8, 'excluding neutrals', recall8, precision8, .8, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a549839",
   "metadata": {},
   "source": [
    "![title](Images/LogRegImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194d5d2",
   "metadata": {
    "id": "b194d5d2"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31875fa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31875fa3",
    "outputId": "e94f264e-b756-42ee-9a9d-85f3efc5483c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: 100 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.997695293546822\n",
      "Testing Data Score: 0.6953515873737833 \n",
      "\n",
      "[[1824 1126  222]\n",
      " [ 495 3302  575]\n",
      " [ 122  809 2518]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.58      0.65      3172\n",
      "           0       0.63      0.76      0.69      4372\n",
      "           1       0.76      0.73      0.74      3449\n",
      "\n",
      "    accuracy                           0.70     10993\n",
      "   macro avg       0.71      0.69      0.69     10993\n",
      "weighted avg       0.70      0.70      0.69     10993\n",
      "\n",
      "Attempt 2: 100 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.99745269286754\n",
      "Testing Data Score: 0.6960160087320356 \n",
      "\n",
      "[[ 913  539  119]\n",
      " [ 273 1645  306]\n",
      " [  69  365 1268]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.58      0.65      1571\n",
      "           0       0.65      0.74      0.69      2224\n",
      "           1       0.75      0.75      0.75      1702\n",
      "\n",
      "    accuracy                           0.70      5497\n",
      "   macro avg       0.71      0.69      0.69      5497\n",
      "weighted avg       0.70      0.70      0.69      5497\n",
      "\n",
      "Attempt 3: 100 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9994906794336356\n",
      "Testing Data Score: 0.851512373968836 \n",
      "\n",
      "[[2696  402]\n",
      " [ 570 2878]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.87      0.85      3098\n",
      "           1       0.88      0.83      0.86      3448\n",
      "\n",
      "    accuracy                           0.85      6546\n",
      "   macro avg       0.85      0.85      0.85      6546\n",
      "weighted avg       0.85      0.85      0.85      6546\n",
      "\n",
      "Attempt 4: 100 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9995416348357525\n",
      "Testing Data Score: 0.8505957836846929 \n",
      "\n",
      "[[1363  203]\n",
      " [ 286 1421]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.87      0.85      1566\n",
      "           1       0.88      0.83      0.85      1707\n",
      "\n",
      "    accuracy                           0.85      3273\n",
      "   macro avg       0.85      0.85      0.85      3273\n",
      "weighted avg       0.85      0.85      0.85      3273\n",
      "\n",
      "Attempt 5: 300 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.9977559437166424\n",
      "Testing Data Score: 0.6967160920585828 \n",
      "\n",
      "[[1821 1111  245]\n",
      " [ 474 3300  631]\n",
      " [ 128  745 2538]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.57      0.65      3177\n",
      "           0       0.64      0.75      0.69      4405\n",
      "           1       0.74      0.74      0.74      3411\n",
      "\n",
      "    accuracy                           0.70     10993\n",
      "   macro avg       0.71      0.69      0.69     10993\n",
      "weighted avg       0.70      0.70      0.70     10993\n",
      "\n",
      "Attempt 6: 300 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.9976801310043668\n",
      "Testing Data Score: 0.6920138257231218 \n",
      "\n",
      "[[ 872  543  120]\n",
      " [ 238 1693  327]\n",
      " [  71  394 1239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.57      0.64      1535\n",
      "           0       0.64      0.75      0.69      2258\n",
      "           1       0.73      0.73      0.73      1704\n",
      "\n",
      "    accuracy                           0.69      5497\n",
      "   macro avg       0.71      0.68      0.69      5497\n",
      "weighted avg       0.70      0.69      0.69      5497\n",
      "\n",
      "Attempt 7: 300 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9993888153203626\n",
      "Testing Data Score: 0.846929422548121 \n",
      "\n",
      "[[2708  415]\n",
      " [ 587 2836]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.87      0.84      3123\n",
      "           1       0.87      0.83      0.85      3423\n",
      "\n",
      "    accuracy                           0.85      6546\n",
      "   macro avg       0.85      0.85      0.85      6546\n",
      "weighted avg       0.85      0.85      0.85      6546\n",
      "\n",
      "Attempt 8: 300 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9995416348357525\n",
      "Testing Data Score: 0.8527344943476932 \n",
      "\n",
      "[[1407  216]\n",
      " [ 266 1384]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.87      0.85      1623\n",
      "           1       0.86      0.84      0.85      1650\n",
      "\n",
      "    accuracy                           0.85      3273\n",
      "   macro avg       0.85      0.85      0.85      3273\n",
      "weighted avg       0.85      0.85      0.85      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "twitter_classi1 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "twitter_classi2 = RandomForestClassifier(n_estimators=300, random_state=0)\n",
    "\n",
    "# models with 100 estimators\n",
    "training_score1, testing_score1, recall1, precision1 = create_model(tweets_df, 1, twitter_classi1, .6, \"100 estimators, 60/40 split, including neutrals\")\n",
    "training_score2, testing_score2, recall2, precision2 = create_model(tweets_df, 2, twitter_classi1, .8, \"100 estimators, 80/20 split, including neutrals\")\n",
    "training_score3, testing_score3, recall3, precision3 = create_model(tweets_no_neut, 3, twitter_classi1, .6, \"100 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score4, testing_score4, recall4, precision4 = create_model(tweets_no_neut, 4, twitter_classi1, .8, \"100 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 1, training_score1, testing_score1, 'including neutrals', recall1, precision1, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 2, training_score2, testing_score2, 'including neutrals', recall2, precision2, .8, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 3, training_score3, testing_score3, 'excluding neutrals', recall3, precision3, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 4, training_score4, testing_score4, 'excluding neutrals', recall4, precision4, .8, 100]\n",
    "\n",
    "# models with 300 estimators\n",
    "training_score5, testing_score5, recall5, precision5 = create_model(tweets_df, 5, twitter_classi2, .6, \"300 estimators, 60/40 split, including neutrals\")\n",
    "training_score6, testing_score6, recall6, precision6 = create_model(tweets_df, 6, twitter_classi2, .8, \"300 estimators, 80/20 split, including neutrals\")\n",
    "training_score7, testing_score7, recall7, precision7 = create_model(tweets_no_neut, 7, twitter_classi2, .6, \"300 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score8, testing_score8, recall8, precision8 = create_model(tweets_no_neut, 8, twitter_classi2, .8, \"300 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 5, training_score5, testing_score5, 'including neutrals', recall5, precision5, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 6, training_score6, testing_score6, 'including neutrals', recall6, precision6, .8, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 7, training_score7, testing_score7, 'excluding neutrals', recall7, precision7, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Random Forest', 8, training_score8, testing_score8, 'excluding neutrals', recall8, precision8, .8, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc7552",
   "metadata": {},
   "source": [
    "![title](Images/RandForImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107093b",
   "metadata": {
    "id": "a107093b"
   },
   "source": [
    "## Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1559f69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1559f69",
    "outputId": "4d763a27-5569-437c-89f8-66fc317c6f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: 100 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.9979985443959243\n",
      "Testing Data Score: 0.7031747475666333 \n",
      "\n",
      "[[1987  895  235]\n",
      " [ 585 3217  657]\n",
      " [ 156  735 2526]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.64      0.68      3117\n",
      "           0       0.66      0.72      0.69      4459\n",
      "           1       0.74      0.74      0.74      3417\n",
      "\n",
      "    accuracy                           0.70     10993\n",
      "   macro avg       0.71      0.70      0.70     10993\n",
      "weighted avg       0.71      0.70      0.70     10993\n",
      "\n",
      "Attempt 2: 100 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.997316229985444\n",
      "Testing Data Score: 0.702746952883391 \n",
      "\n",
      "[[ 959  486  114]\n",
      " [ 284 1592  319]\n",
      " [  75  356 1312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.62      0.67      1559\n",
      "           0       0.65      0.73      0.69      2195\n",
      "           1       0.75      0.75      0.75      1743\n",
      "\n",
      "    accuracy                           0.70      5497\n",
      "   macro avg       0.71      0.70      0.70      5497\n",
      "weighted avg       0.71      0.70      0.70      5497\n",
      "\n",
      "Attempt 3: 100 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9995925435469084\n",
      "Testing Data Score: 0.8481515429269783 \n",
      "\n",
      "[[2656  457]\n",
      " [ 537 2896]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.85      0.84      3113\n",
      "           1       0.86      0.84      0.85      3433\n",
      "\n",
      "    accuracy                           0.85      6546\n",
      "   macro avg       0.85      0.85      0.85      6546\n",
      "weighted avg       0.85      0.85      0.85      6546\n",
      "\n",
      "Attempt 4: 100 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9995416348357525\n",
      "Testing Data Score: 0.8612893369996945 \n",
      "\n",
      "[[1383  208]\n",
      " [ 246 1436]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.87      0.86      1591\n",
      "           1       0.87      0.85      0.86      1682\n",
      "\n",
      "    accuracy                           0.86      3273\n",
      "   macro avg       0.86      0.86      0.86      3273\n",
      "weighted avg       0.86      0.86      0.86      3273\n",
      "\n",
      "Attempt 5: 300 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.9979985443959243\n",
      "Testing Data Score: 0.7077230965159647 \n",
      "\n",
      "[[1842  903  235]\n",
      " [ 588 3266  624]\n",
      " [ 157  706 2672]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.62      0.66      2980\n",
      "           0       0.67      0.73      0.70      4478\n",
      "           1       0.76      0.76      0.76      3535\n",
      "\n",
      "    accuracy                           0.71     10993\n",
      "   macro avg       0.71      0.70      0.71     10993\n",
      "weighted avg       0.71      0.71      0.71     10993\n",
      "\n",
      "Attempt 6: 300 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.9976346433770015\n",
      "Testing Data Score: 0.7118428233581954 \n",
      "\n",
      "[[ 999  465  110]\n",
      " [ 282 1626  326]\n",
      " [  64  337 1288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.63      0.68      1574\n",
      "           0       0.67      0.73      0.70      2234\n",
      "           1       0.75      0.76      0.75      1689\n",
      "\n",
      "    accuracy                           0.71      5497\n",
      "   macro avg       0.72      0.71      0.71      5497\n",
      "weighted avg       0.71      0.71      0.71      5497\n",
      "\n",
      "Attempt 7: 300 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9996944076601814\n",
      "Testing Data Score: 0.852581729300336 \n",
      "\n",
      "[[2677  421]\n",
      " [ 544 2904]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.86      0.85      3098\n",
      "           1       0.87      0.84      0.86      3448\n",
      "\n",
      "    accuracy                           0.85      6546\n",
      "   macro avg       0.85      0.85      0.85      6546\n",
      "weighted avg       0.85      0.85      0.85      6546\n",
      "\n",
      "Attempt 8: 300 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9996180290297937\n",
      "Testing Data Score: 0.867399938893981 \n",
      "\n",
      "[[1387  187]\n",
      " [ 247 1452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.88      0.86      1574\n",
      "           1       0.89      0.85      0.87      1699\n",
      "\n",
      "    accuracy                           0.87      3273\n",
      "   macro avg       0.87      0.87      0.87      3273\n",
      "weighted avg       0.87      0.87      0.87      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "twitter_ex1 = ExtraTreesClassifier(n_estimators=100, random_state=1)\n",
    "twitter_ex2 = ExtraTreesClassifier(n_estimators=300, random_state=1)\n",
    "\n",
    "# models with 100 estimators\n",
    "training_score1, testing_score1, recall1, precision1 = create_model(tweets_df, 1, twitter_ex1, .6, \"100 estimators, 60/40 split, including neutrals\")\n",
    "training_score2, testing_score2, recall2, precision2 = create_model(tweets_df, 2, twitter_ex1, .8, \"100 estimators, 80/20 split, including neutrals\")\n",
    "training_score3, testing_score3, recall3, precision3 = create_model(tweets_no_neut, 3, twitter_ex1, .6, \"100 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score4, testing_score4, recall4, precision4 = create_model(tweets_no_neut, 4, twitter_ex1, .8, \"100 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 1, training_score1, testing_score1, 'including neutrals', recall1, precision1, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 2, training_score2, testing_score2, 'including neutrals', recall2, precision2, .8, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 3, training_score3, testing_score3, 'excluding neutrals', recall3, precision3, .6, 100]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 4, training_score4, testing_score4, 'excluding neutrals', recall4, precision4, .8, 100]\n",
    "\n",
    "# models with 300 estimators\n",
    "training_score5, testing_score5, recall5, precision5 = create_model(tweets_df, 5, twitter_ex2, .6, \"300 estimators, 60/40 split, including neutrals\")\n",
    "training_score6, testing_score6, recall6, precision6 = create_model(tweets_df, 6, twitter_ex2, .8, \"300 estimators, 80/20 split, including neutrals\")\n",
    "training_score7, testing_score7, recall7, precision7 = create_model(tweets_no_neut, 7, twitter_ex2, .6, \"300 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score8, testing_score8, recall8, precision8 = create_model(tweets_no_neut, 8, twitter_ex2, .8, \"300 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 5, training_score5, testing_score5, 'including neutrals', recall5, precision5, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 6, training_score6, testing_score6, 'including neutrals', recall6, precision6, .8, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 7, training_score7, testing_score7, 'excluding neutrals', recall7, precision7, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Extreme Trees', 8, training_score8, testing_score8, 'excluding neutrals', recall8, precision8, .8, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cac28",
   "metadata": {},
   "source": [
    "![title](Images/ExtTreeImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002db64",
   "metadata": {
    "id": "0002db64"
   },
   "source": [
    "## Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772d9f0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "772d9f0b",
    "outputId": "c48e6440-ffae-4c48-e051-81efb218505a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: 50 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.6555070354196991\n",
      "Testing Data Score: 0.6486855271536432 \n",
      "\n",
      "[[1232 1668  202]\n",
      " [ 296 3633  498]\n",
      " [ 101 1097 2266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.40      0.52      3102\n",
      "           0       0.57      0.82      0.67      4427\n",
      "           1       0.76      0.65      0.70      3464\n",
      "\n",
      "    accuracy                           0.65     10993\n",
      "   macro avg       0.70      0.62      0.63     10993\n",
      "weighted avg       0.68      0.65      0.64     10993\n",
      "\n",
      "Attempt 2: 50 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.654839883551674\n",
      "Testing Data Score: 0.6487174822630526 \n",
      "\n",
      "[[ 609  907  112]\n",
      " [ 126 1803  246]\n",
      " [  43  497 1154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.37      0.51      1628\n",
      "           0       0.56      0.83      0.67      2175\n",
      "           1       0.76      0.68      0.72      1694\n",
      "\n",
      "    accuracy                           0.65      5497\n",
      "   macro avg       0.70      0.63      0.63      5497\n",
      "weighted avg       0.69      0.65      0.64      5497\n",
      "\n",
      "Attempt 3: 50 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.7999388815320363\n",
      "Testing Data Score: 0.7991139627253284 \n",
      "\n",
      "[[2909  306]\n",
      " [1009 2322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.90      0.82      3215\n",
      "           1       0.88      0.70      0.78      3331\n",
      "\n",
      "    accuracy                           0.80      6546\n",
      "   macro avg       0.81      0.80      0.80      6546\n",
      "weighted avg       0.81      0.80      0.80      6546\n",
      "\n",
      "Attempt 4: 50 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.8068754774637128\n",
      "Testing Data Score: 0.7952948365413993 \n",
      "\n",
      "[[1379  135]\n",
      " [ 535 1224]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.91      0.80      1514\n",
      "           1       0.90      0.70      0.79      1759\n",
      "\n",
      "    accuracy                           0.80      3273\n",
      "   macro avg       0.81      0.80      0.79      3273\n",
      "weighted avg       0.82      0.80      0.79      3273\n",
      "\n",
      "Attempt 5: 300 estimators, 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.7122149442018437\n",
      "Testing Data Score: 0.673428545438006 \n",
      "\n",
      "[[1680 1281  192]\n",
      " [ 470 3459  508]\n",
      " [ 138 1001 2264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.53      0.62      3153\n",
      "           0       0.60      0.78      0.68      4437\n",
      "           1       0.76      0.67      0.71      3403\n",
      "\n",
      "    accuracy                           0.67     10993\n",
      "   macro avg       0.70      0.66      0.67     10993\n",
      "weighted avg       0.69      0.67      0.67     10993\n",
      "\n",
      "Attempt 6: 300 estimators, 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.7148835516739447\n",
      "Testing Data Score: 0.6881935601237038 \n",
      "\n",
      "[[ 821  623   84]\n",
      " [ 230 1748  255]\n",
      " [  66  456 1214]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.54      0.62      1528\n",
      "           0       0.62      0.78      0.69      2233\n",
      "           1       0.78      0.70      0.74      1736\n",
      "\n",
      "    accuracy                           0.69      5497\n",
      "   macro avg       0.71      0.67      0.68      5497\n",
      "weighted avg       0.70      0.69      0.69      5497\n",
      "\n",
      "Attempt 7: 300 estimators, 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.8838749108689009\n",
      "Testing Data Score: 0.8420409410326917 \n",
      "\n",
      "[[2695  468]\n",
      " [ 566 2817]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.85      0.84      3163\n",
      "           1       0.86      0.83      0.84      3383\n",
      "\n",
      "    accuracy                           0.84      6546\n",
      "   macro avg       0.84      0.84      0.84      6546\n",
      "weighted avg       0.84      0.84      0.84      6546\n",
      "\n",
      "Attempt 8: 300 estimators, 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.8726508785332314\n",
      "Testing Data Score: 0.844179651695692 \n",
      "\n",
      "[[1360  225]\n",
      " [ 285 1403]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.86      0.84      1585\n",
      "           1       0.86      0.83      0.85      1688\n",
      "\n",
      "    accuracy                           0.84      3273\n",
      "   macro avg       0.84      0.84      0.84      3273\n",
      "weighted avg       0.84      0.84      0.84      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "twitter_ada1 = AdaBoostClassifier(n_estimators=50, random_state=1)\n",
    "twitter_ada2 = AdaBoostClassifier(n_estimators=300, random_state=1)\n",
    "\n",
    "\n",
    "# models with 50 estimators\n",
    "training_score1, testing_score1, recall1, precision1 = create_model(tweets_df, 1, twitter_ada1, .6, \"50 estimators, 60/40 split, including neutrals\")\n",
    "training_score2, testing_score2, recall2, precision2 = create_model(tweets_df, 2, twitter_ada1, .8, \"50 estimators, 80/20 split, including neutrals\")\n",
    "training_score3, testing_score3, recall3, precision3 = create_model(tweets_no_neut, 3, twitter_ada1, .6, \"50 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score4, testing_score4, recall4, precision4 = create_model(tweets_no_neut, 4, twitter_ada1, .8, \"50 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 1, training_score1, testing_score1, 'including neutrals', recall1, precision1, .6, 50]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 2, training_score2, testing_score2, 'including neutrals', recall2, precision2, .8, 50]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 3, training_score3, testing_score3, 'excluding neutrals', recall3, precision3, .6, 50]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 4, training_score4, testing_score4, 'excluding neutrals', recall4, precision4, .8, 50]\n",
    "\n",
    "# models with 300 estimators\n",
    "training_score5, testing_score5, recall5, precision5 = create_model(tweets_df, 5, twitter_ada2, .6, \"300 estimators, 60/40 split, including neutrals\")\n",
    "training_score6, testing_score6, recall6, precision6 = create_model(tweets_df, 6, twitter_ada2, .8, \"300 estimators, 80/20 split, including neutrals\")\n",
    "training_score7, testing_score7, recall7, precision7 = create_model(tweets_no_neut, 7, twitter_ada2, .6, \"300 estimators, 60/40 split, excluding neutrals\")\n",
    "training_score8, testing_score8, recall8, precision8 = create_model(tweets_no_neut, 8, twitter_ada2, .8, \"300 estimators, 80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 5, training_score5, testing_score5, 'including neutrals', recall5, precision5, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 6, training_score6, testing_score6, 'including neutrals', recall6, precision6, .8, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 7, training_score7, testing_score7, 'excluding neutrals', recall7, precision7, .6, 300]\n",
    "performance_df.loc[len(performance_df.index)] = ['Ada Boost', 8, training_score8, testing_score8, 'excluding neutrals', recall8, precision8, .8, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb5bd7",
   "metadata": {},
   "source": [
    "![title](Images/AdaBoostImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a8104",
   "metadata": {
    "id": "a25a8104"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dfd0224",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dfd0224",
    "outputId": "41824676-3c97-4853-da47-3e68628c670f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: 60/40 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.8054342552159146\n",
      "Testing Data Score: 0.6207586646047485 \n",
      "\n",
      "[[1226 1737  147]\n",
      " [ 309 3647  470]\n",
      " [  75 1431 1951]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.39      0.52      3110\n",
      "           0       0.54      0.82      0.65      4426\n",
      "           1       0.76      0.56      0.65      3457\n",
      "\n",
      "    accuracy                           0.62     10993\n",
      "   macro avg       0.69      0.59      0.61     10993\n",
      "weighted avg       0.67      0.62      0.61     10993\n",
      "\n",
      "Attempt 2: 80/20 split, including neutrals ----------------------------------------\n",
      "Training Data Score: 0.7927128820960698\n",
      "Testing Data Score: 0.6150627615062761 \n",
      "\n",
      "[[ 621  910   73]\n",
      " [ 131 1799  240]\n",
      " [  33  729  961]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.39      0.52      1604\n",
      "           0       0.52      0.83      0.64      2170\n",
      "           1       0.75      0.56      0.64      1723\n",
      "\n",
      "    accuracy                           0.62      5497\n",
      "   macro avg       0.69      0.59      0.60      5497\n",
      "weighted avg       0.67      0.62      0.61      5497\n",
      "\n",
      "Attempt 3: 60/40 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.9460120199653662\n",
      "Testing Data Score: 0.8501374885426215 \n",
      "\n",
      "[[2496  611]\n",
      " [ 370 3069]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.80      0.84      3107\n",
      "           1       0.83      0.89      0.86      3439\n",
      "\n",
      "    accuracy                           0.85      6546\n",
      "   macro avg       0.85      0.85      0.85      6546\n",
      "weighted avg       0.85      0.85      0.85      6546\n",
      "\n",
      "Attempt 4: 80/20 split, excluding neutrals ----------------------------------------\n",
      "Training Data Score: 0.939572192513369\n",
      "Testing Data Score: 0.8539566147265506 \n",
      "\n",
      "[[1257  280]\n",
      " [ 198 1538]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.82      0.84      1537\n",
      "           1       0.85      0.89      0.87      1736\n",
      "\n",
      "    accuracy                           0.85      3273\n",
      "   macro avg       0.85      0.85      0.85      3273\n",
      "weighted avg       0.85      0.85      0.85      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "\n",
    "training_score1, testing_score1, recall1, precision1 = create_model(tweets_df, 1, mnb, .6, \"60/40 split, including neutrals\")\n",
    "training_score2, testing_score2, recall2, precision2 = create_model(tweets_df, 2, mnb, .8, \"80/20 split, including neutrals\")\n",
    "training_score3, testing_score3, recall3, precision3 = create_model(tweets_no_neut, 3, mnb, .6, \"60/40 split, excluding neutrals\")\n",
    "training_score4, testing_score4, recall4, precision4 = create_model(tweets_no_neut, 4, mnb, .8, \"80/20 split, excluding neutrals\")\n",
    "\n",
    "performance_df.loc[len(performance_df.index)] = ['Multinomial Naive Bayes', 1, training_score1, testing_score1, 'including neutrals', recall1, precision1, .6, 0]\n",
    "performance_df.loc[len(performance_df.index)] = ['Multinomial Naive Bayes', 2, training_score2, testing_score2, 'including neutrals', recall2, precision2, .8, 0]\n",
    "performance_df.loc[len(performance_df.index)] = ['Multinomial Naive Bayes', 3, training_score3, testing_score3, 'excluding neutrals', recall3, precision3, .6, 0]\n",
    "performance_df.loc[len(performance_df.index)] = ['Multinomial Naive Bayes', 4, training_score4, testing_score4, 'excluding neutrals', recall4, precision4, .8, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068f9aa",
   "metadata": {},
   "source": [
    "![title](Images/NBImage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "w0CASu0kjjw9",
   "metadata": {
    "id": "w0CASu0kjjw9"
   },
   "outputs": [],
   "source": [
    "performance_df.to_csv(\"Resources/performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844353c",
   "metadata": {},
   "source": [
    "![title](Images/Sentiment_Analysis_Summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54lxdKyCkOTX",
   "metadata": {
    "id": "54lxdKyCkOTX"
   },
   "source": [
    "### MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "UCtmN_uYkQN0",
   "metadata": {
    "id": "UCtmN_uYkQN0"
   },
   "outputs": [],
   "source": [
    "performance_neutral = performance_df[performance_df[\"Parameters\"] == \"including neutrals\"]\n",
    "performance_no_neutral = performance_df[performance_df[\"Parameters\"] == \"excluding neutrals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a_JhV52kSW1",
   "metadata": {
    "id": "1a_JhV52kSW1"
   },
   "outputs": [],
   "source": [
    "# with neutrals\n",
    "labels = performance_neutral[\"Model\"].tolist()\n",
    "training = performance_neutral[\"Training Score\"].tolist()\n",
    "testing = performance_neutral[\"Testing Score\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "OH9IqtSGkTkt",
   "metadata": {
    "id": "OH9IqtSGkTkt"
   },
   "outputs": [],
   "source": [
    "training = [round(x, 2) for x in training]\n",
    "testing = [round(x, 2) for x in testing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0DzfdE4tkU9t",
   "metadata": {
    "id": "0DzfdE4tkU9t"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# x = np.arange(len(labels))\n",
    "# width = 0.35\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# rects1 = ax.bar(x - width/2, training, width, label='Training')\n",
    "# rects2 = ax.bar(x + width/2, testing, width, label='Testing')\n",
    "\n",
    "# ax.set_ylabel('Score')\n",
    "# ax.set_xticks(x, labels)\n",
    "# ax.xticks(rotation=45)\n",
    "# ax.set_title('Scores of Models including Neutral Tweets')\n",
    "# ax.legend()\n",
    "\n",
    "# ax.bar_label(rects1, padding=3)\n",
    "# ax.bar_label(rects2, padding=3)\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
